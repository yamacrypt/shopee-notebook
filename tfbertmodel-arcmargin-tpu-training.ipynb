{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I cannot train model properly.......","metadata":{}},{"cell_type":"markdown","source":"### Reference\n> @ragnar123 [bert-baseline](https://www.kaggle.com/ragnar123/bert-baseline)\n\nThank you for sharing","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nfrom transformers import AutoTokenizer, TFAutoModel \nwarnings.simplefilter('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nEPOCHS = 20\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nTFM_PATH = 'bert-base-uncased'\nTOKENIZER_PATH = 'bert-base-uncased'\nLR = 1e-3\n\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.python.keras.utils import tf_utils\n\n\ndef _resolve_training(layer, training):\n    if training is None:\n        training = K.learning_phase()\n    if isinstance(training, int):\n        training = bool(training)\n    if not layer.trainable:\n        # When the layer is not trainable, override the value\n        training = False\n    return training#tf_utils.constant_value(training)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers\nclass AdaCos(Layer):\n    \"\"\"\n    Implementation of AdaCos layer. Reference: https://arxiv.org/abs/1905.00292\n    \n    Arguments:\n      num_classes: number of classes to classify\n      is_dynamic: if False, use Fixed AdaCos. Else, use Dynamic Adacos.\n      regularizer: weights regularizer\n    \"\"\"\n    def __init__(self,\n                 num_classes,\n                 is_dynamic=True,\n                 regularizer=None,\n                 **kwargs):\n\n        super().__init__(**kwargs)\n        self._n_classes = num_classes\n        self._init_s = math.sqrt(2) * math.log(num_classes - 1)\n        self._is_dynamic = is_dynamic\n        self._regularizer = regularizer\n\n    def build(self, input_shape):\n        embedding_shape, label_shape = input_shape\n        self._w = self.add_weight(name='W',shape=(embedding_shape[-1], self._n_classes),\n                                  initializer='glorot_uniform',\n                                  trainable=True,\n                                  regularizer=self._regularizer)\n        if self._is_dynamic:\n            self._s = self.add_weight(name='S',shape=(),\n                                      initializer=Constant(self._init_s),\n                                      trainable=False,\n                                      aggregation=tf.VariableAggregation.MEAN)\n\n    def call(self, inputs, training=None):\n        embedding, label = inputs\n        label = tf.cast(label, dtype=tf.int32)\n\n        # Squeezing is necessary for Keras. It expands the dimension to (n, 1)\n        label = tf.reshape(label, [-1])\n\n        # Normalize features and weights and compute dot product\n        x = tf.nn.l2_normalize(embedding, axis=1)\n        w = tf.nn.l2_normalize(self._w, axis=0)\n        logits = tf.matmul(x, w)\n\n        # Fixed AdaCos\n        is_dynamic = self._is_dynamic#tf_utils.constant_value(self._is_dynamic)\n        if not is_dynamic:\n            # _s is not created since we are not in dynamic mode\n            output = tf.multiply(self._init_s, logits)\n            return output\n\n        training = _resolve_training(self, training)\n        if not training:\n            # We don't have labels to update _s if we're not in training mode\n            return self._s * logits\n        else:\n            theta = tf.math.acos(\n                    K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n            one_hot = tf.one_hot(label, depth=self._n_classes)\n            b_avg = tf.where(one_hot < 1.0,\n                             tf.exp(self._s * logits),\n                             tf.zeros_like(logits))\n            b_avg = tf.reduce_mean(tf.reduce_sum(b_avg, axis=1))\n            theta_class = tf.gather_nd(\n                    theta,\n                    tf.stack([\n                        tf.range(tf.shape(label)[0]),\n                        tf.cast(label, tf.int32)\n                    ], axis=1))\n            mid_index = tf.shape(theta_class)[0] // 2 + 1\n            theta_med = tf.nn.top_k(theta_class, mid_index).values[-1]\n\n            # Since _s is not trainable, this assignment is safe. Also,\n            # tf.function ensures that this will run in the right order.\n            self._s.assign(\n                    tf.math.log(b_avg) /\n                    tf.math.cos(tf.minimum(math.pi/4, theta_med)))\n\n            # Return scaled logits\n            return self._s * logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\nseed_everything(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\nN_CLASSES = train_df['label_group'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#b=TFAutoModel.from_pretrained(TFM_PATH)\n#b.save_pretrained('base_bert')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\ntokenizer.save_pretrained('tokenizer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_df():\n    train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\n    train_df['label_group'] = LabelEncoder().fit_transform(train_df['label_group'])\n    N_CLASSES = train_df['label_group'].nunique()\n    train_x, valid_x = train_test_split(train_df[['title', 'label_group']], shuffle=True, stratify=train_df['label_group'], random_state=SEED, test_size=0.33)\n    return train_x, valid_x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(df):\n    inputs = tokenizer(df.title.tolist(), return_tensors='tf', max_length=64, padding='max_length', truncation=True)\n    return inputs['input_ids'].numpy(), inputs['attention_mask'].numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_ds(tokens, masks, labels, mode='train'):\n    text_ds = tf.data.Dataset.from_tensor_slices((tokens, masks, labels))\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    ds = tf.data.Dataset.zip((text_ds, label_ds))\n    if mode == 'train':\n        ds = ds.repeat()\n        ds = ds.shuffle(len(tokens))\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTO)\n    return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load():\n    train_df, valid_df = load_df()\n    STEPS_PER_EPOCH = train_df.shape[0] // BATCH_SIZE\n    if train_df.shape[0] % BATCH_SIZE != 0: STEPS_PER_EPOCH += 1\n    train_x, valid_x = tokenize(train_df), tokenize(valid_df)\n    train_ds, valid_ds = load_ds(*train_x, train_df.label_group.values), load_ds(*valid_x, valid_df.label_group.values, mode='valid')\n    return train_ds, valid_ds, STEPS_PER_EPOCH","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Jannoshh/simple-sam\n!mv simple-sam sam\nimport sam\nimport sam.sam\nfrom sam.sam import sam_train_step","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaArcFace(tf.keras.Model):\n    def __init__(self,rho=0.05):\n        super().__init__()\n        self.roberta = TFAutoModel.from_pretrained(TFM_PATH)\n        adacos=AdaCos(N_CLASSES, name='AdaCos' )    \n        self.arc_margin =adacos\n        self.softmax = tf.keras.layers.Softmax(dtype='float32')\n        self.rho = tf.constant(rho, dtype='float32')\n    def train_step(self, data):\n        return sam_train_step(self, data)\n    def call(self, inputs):\n        tokens, masks, labels = inputs\n        out = self.roberta(tokens, masks)\n        feats = out.last_hidden_state[:, 0, :]\n        out = self.arc_margin((feats, labels),True)\n        out = self.softmax(out)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass SAMModel(tf.keras.models.Model):\n    def train_step(self, data):\n        return sam_train_step(self, data)\ndef build_model():\n    #model = RobertaArcFace()\n    max_len = 64\n    roberta=TFAutoModel.from_pretrained(TFM_PATH)\n    adacos=AdaCos(N_CLASSES, name='AdaCos' ) \n    softmax= tf.keras.layers.Softmax(dtype='float32')\n    tokens = tf.keras.layers.Input(shape = (max_len,), dtype=tf.int32, name = 'tokens')\n    masks  = tf.keras.layers.Input(shape = (max_len,), dtype=tf.int32, name = 'masks')\n    labels = tf.keras.layers.Input(shape = (), name = 'label')\n    out = roberta(tokens, masks)\n    feats = out.last_hidden_state[:, 0, :]\n    out = adacos((feats, labels),False)\n    out = softmax(out)\n    model = SAMModel(inputs = [tokens,masks, labels], outputs = [out])\n    radam = tfa.optimizers.RectifiedAdam()\n    base_optimizer = tf.keras.optimizers.SGD()  # define an optimizer for the \"sharpness-aware\" update\n    #sam_optimizer = sam.sam.SAM(base_optimizer)\n    model.compile(optimizer=radam,\n                  loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\n                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n# Function for a custom learning rate scheduler with warmup and decay\ndef get_lr_callback():\n    lr_start   = 0.000001\n    lr_max     = 0.000005 * BATCH_SIZE\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max    \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n    return lr_callback\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    train_ds, valid_ds, STEPS_PER_EPOCH = load()\n    checkpoint = ModelCheckpoint(\n        f'bert-arcface.h5', \n        monitor = 'val_loss', \n        save_best_only = True,\n        save_weights_only = True, \n        mode = 'min'\n    )\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=1, min_lr=0.00001,verbose=1)\n   \n    early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                              patience = 3, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\n    \n    with strategy.scope():\n        model = build_model()\n    model.fit(\n        train_ds,\n        validation_data=valid_ds,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=[checkpoint,early_stop,reduce_lr ]\n    )\n    #model.save(\"hoge\", save_format=\"tf\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip base_bert.zip ./base_bert -r","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nos.chdir('/kaggle/working')\nfrom IPython.display import FileLink \nFileLink('./base_bert.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m=tf.keras.models.load_model('./bert-arcface.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold,StratifiedKFold\nimport math, re, os\nimport gc\nimport tensorflow as tf\n#from tensorflow.keras.layers import *\nimport numpy as np\nfrom tensorflow import keras\nfrom functools import partial\n\nfrom tensorflow.keras.applications.imagenet_utils import decode_predictions\nimport tensorflow_addons as tfa\nFOLD=4\nkf = KFold(n_splits=FOLD, shuffle=True, random_state = SEED)\n#GCS_PATH = '/kaggle/input/cassava-leaf-disease-tfrecords-center-512x512'\ni=0\n#for n_fold, (train, test) in enumerate(cv.split(X)):\ntarget=train_df[['title', 'label_group']]\ndef load_df():\n    train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\n    train_df['label_group'] = LabelEncoder().fit_transform(train_df['label_group'])\n    N_CLASSES = train_df['label_group'].nunique()\n    train_x, valid_x = train_test_split(train_df[['title', 'label_group']], shuffle=True, stratify=train_df['label_group'], random_state=SEED, test_size=0.33)\n    return train_x, valid_x\nfor train_index, test_index in kf.split(target):\n    K.clear_session()\n    gc.collect()\n    train=target.iloc[train_index]\n    valid=target.iloc[test_index]\n    STEPS_PER_EPOCH = train.shape[0]  // BATCH_SIZE\n    if train_df.shape[0] % BATCH_SIZE != 0: STEPS_PER_EPOCH += 1\n    train_x, valid_x = tokenize(train), tokenize(valid)\n    train_ds, valid_ds = load_ds(*train_x, train.label_group.values), load_ds(*valid_x, valid.label_group.values, mode='valid')\n    del train_x,valid_x,train,valid\n    print(f'fold={i}')\n    checkpoint = ModelCheckpoint(\n        f'bert-adacos_fold{i}.h5', \n        monitor = 'val_loss', \n        save_best_only = True,\n        save_weights_only = True, \n        mode = 'min'\n    )\n    #reduce_lr = ReduceLROnPlateau()\n    early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                              patience = 3, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\n    \n    with strategy.scope():\n        model = build_model()\n    model.fit(\n        train_ds,\n        validation_data=valid_ds,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=[checkpoint,early_stop,get_lr_callback() ]\n    )\n    del model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}